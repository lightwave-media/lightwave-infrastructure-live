# CineOS Production Infrastructure

Production deployment configuration for CineOS - Cinema Management SaaS Platform.

## Stack Components

1. **S3 Bucket** - Wagtail media storage (REQUIRED for ephemeral ECS containers)
2. **PostgreSQL RDS** - Multi-AZ database with backups
3. **Redis ElastiCache** - Multi-AZ cache and Celery broker
4. **Django ECS Fargate** - Auto-scaling CineOS application
5. **Cloudflare DNS** - DDoS protection and SSL termination for cineos.io

## Architecture

```
Internet
    ↓
Cloudflare (cineos.io, www.cineos.io)
  - DDoS Protection
  - WAF
  - SSL/TLS
  - CDN for media via S3
    ↓
AWS Application Load Balancer
    ↓
ECS Fargate (2 containers)
  - Django 5.2+ (Pegasus)
  - Wagtail CMS
  - Gunicorn
  - Celery worker + beat
    ↓
┌─────────────┬──────────────┬──────────────┐
│ PostgreSQL  │ Redis        │ S3 Bucket    │
│ (Multi-AZ)  │ (Multi-AZ)   │ (Media)      │
└─────────────┴──────────────┴──────────────┘
```

## Key Differences from Standard Django Stack

**Wagtail CMS Requirement**: CineOS uses Wagtail CMS which requires persistent media storage. Since ECS Fargate containers are ephemeral (no persistent file system), **S3 is REQUIRED** for production.

**Media Flow**:
1. Wagtail admin uploads media → Django → S3 bucket
2. Public access → Cloudflare CDN → S3 bucket
3. Django serves media URLs pointing to S3/CloudFront

## Prerequisites

### 1. AWS Resources

#### VPC and Subnets
```bash
export VPC_ID="vpc-02f48c62006cacfae"
export PRIVATE_SUBNET_IDS="subnet-00e39a8d07f4c256b,subnet-0ba0de978370667c6"
export PUBLIC_SUBNET_IDS="<public-subnet-1>,<public-subnet-2>" # Required for ALB
```

#### ECR Repository
```bash
# Create ECR repository for CineOS
aws ecr create-repository \
  --repository-name cineos \
  --profile lightwave-admin-new

# Output: ECR_REPOSITORY_URL
# Example: 738605694078.dkr.ecr.us-east-1.amazonaws.com/cineos
```

### 2. AWS Secrets Manager

Secrets are already created:

```bash
# List all cineos secrets
aws secretsmanager list-secrets \
  --profile lightwave-admin-new \
  --query 'SecretList[?contains(Name, `/lightwave/prod/cineos/`)].Name'

# Output:
# - /lightwave/prod/cineos/cloudflare-zone-id (8dbc36291fa92868de35f70263c04b81)
# - /lightwave/prod/cineos/secret-key (Django SECRET_KEY)
# - /lightwave/prod/cineos/database-password (PostgreSQL password)
# - /lightwave/prod/cineos/allowed-hosts (cineos.io,www.cineos.io,api.cineos.io)
# - /lightwave/prod/cineos/s3-media-bucket (cineos-media-prod)
```

### 3. Environment Variables

Set these in GitHub Actions secrets or local environment:

```bash
# Required
export VPC_ID="vpc-02f48c62006cacfae"
export PRIVATE_SUBNET_IDS="subnet-00e39a8d07f4c256b,subnet-0ba0de978370667c6"
export PUBLIC_SUBNET_IDS="<public-subnet-1>,<public-subnet-2>"
export ECR_REPOSITORY_URL="738605694078.dkr.ecr.us-east-1.amazonaws.com/cineos"
export IMAGE_TAG="prod"
export AWS_REGION="us-east-1"

# Secrets Manager ARNs
export DJANGO_SECRET_KEY_ARN="arn:aws:secretsmanager:us-east-1:738605694078:secret:/lightwave/prod/cineos/secret-key-bCxnFV"
export DB_MASTER_PASSWORD="<retrieve-from-secrets-manager>"
export CLOUDFLARE_ZONE_ID="8dbc36291fa92868de35f70263c04b81"
export CLOUDFLARE_API_TOKEN="<retrieve-from-secrets-manager>"

# Django configuration
export DJANGO_ALLOWED_HOSTS="cineos.io,www.cineos.io,*.amazonaws.com"
export S3_MEDIA_BUCKET_NAME="cineos-media-prod"

# Auto-generated (after django service deploys)
export ALB_DNS_NAME="<auto-generated-by-django-service>"
```

## Deployment

### Option 1: Manual Deployment

```bash
# 1. Build and push Docker image
cd /Users/joelschaeffer/dev/lightwave-workspace/domains/cineos.io
docker build --platform linux/arm64 -t cineos:prod -f Dockerfile.prod .

# 2. Push to ECR
aws ecr get-login-password --region us-east-1 --profile lightwave-admin-new | \
  docker login --username AWS --password-stdin 738605694078.dkr.ecr.us-east-1.amazonaws.com
docker tag cineos:prod 738605694078.dkr.ecr.us-east-1.amazonaws.com/cineos:prod
docker push 738605694078.dkr.ecr.us-east-1.amazonaws.com/cineos:prod

# 3. Deploy infrastructure
cd /Users/joelschaeffer/dev/lightwave-workspace/Infrastructure/lightwave-infrastructure-live/prod/us-east-1/cineos
terragrunt stack plan
terragrunt stack apply
```

### Option 2: GitHub Actions (GitOps)

1. Push infrastructure configuration to main branch
2. Trigger deployment via GitHub Actions workflow
3. Workflow will:
   - Build Docker image for ARM64
   - Push to ECR
   - Run `terragrunt stack apply`
   - Wait for health checks

## Post-Deployment

### 1. Run Database Migrations

```bash
# Get ECS task ARN
TASK_ARN=$(aws ecs list-tasks \
  --cluster cineos-prod \
  --query 'taskArns[0]' \
  --output text \
  --profile lightwave-admin-new)

# Run migrations
aws ecs execute-command \
  --cluster cineos-prod \
  --task $TASK_ARN \
  --container django \
  --interactive \
  --command "python manage.py migrate" \
  --profile lightwave-admin-new
```

### 2. Create Django Superuser

```bash
aws ecs execute-command \
  --cluster cineos-prod \
  --task $TASK_ARN \
  --container django \
  --interactive \
  --command "python manage.py createsuperuser" \
  --profile lightwave-admin-new
```

### 3. Collect Static Files (to S3)

```bash
aws ecs execute-command \
  --cluster cineos-prod \
  --task $TASK_ARN \
  --container django \
  --interactive \
  --command "python manage.py collectstatic --noinput" \
  --profile lightwave-admin-new
```

### 4. Test Wagtail Admin

```bash
# Test health endpoints
curl https://cineos.io/health/live/
curl https://cineos.io/health/ready/

# Access Wagtail admin
open https://cineos.io/admin/
```

## S3 Media Storage Configuration

### IAM Policy for ECS Task Role

The Django ECS task needs permissions to read/write S3:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:DeleteObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::cineos-media-prod",
        "arn:aws:s3:::cineos-media-prod/*"
      ]
    }
  ]
}
```

### Django Settings for S3

In production (`cineos_config/settings.py`):

```python
USE_S3_MEDIA = env.bool("USE_S3_MEDIA", default=True)

if USE_S3_MEDIA:
    AWS_STORAGE_BUCKET_NAME = env("AWS_STORAGE_BUCKET_NAME", default="cineos-media-prod")
    AWS_S3_REGION_NAME = env("AWS_S3_REGION_NAME", default="us-east-1")
    AWS_S3_CUSTOM_DOMAIN = f"{AWS_STORAGE_BUCKET_NAME}.s3.amazonaws.com"

    STORAGES["default"] = {
        "BACKEND": "storages.backends.s3boto3.S3Boto3Storage",
        "OPTIONS": {
            "bucket_name": AWS_STORAGE_BUCKET_NAME,
            "region_name": AWS_S3_REGION_NAME,
            "custom_domain": AWS_S3_CUSTOM_DOMAIN,
            "querystring_auth": False,  # Public read
        },
    }
```

## Monitoring

### CloudWatch Logs

```bash
# Django application logs
aws logs tail /ecs/cineos-prod --follow --profile lightwave-admin-new

# PostgreSQL logs
aws logs tail /aws/rds/instance/cineos-prod/postgresql --follow

# Redis logs
aws logs tail /aws/elasticache/cineos-prod/slow-log --follow
```

### CloudWatch Metrics

Key metrics to monitor:
- **ECS**: CPUUtilization, MemoryUtilization, TargetResponseTime
- **RDS**: DatabaseConnections, CPUUtilization, FreeStorageSpace
- **Redis**: CacheHits, CacheMisses, Evictions
- **ALB**: TargetResponseTime, HTTPCode_Target_5XX_Count
- **S3**: BucketSizeBytes, NumberOfObjects, AllRequests

### Cloudflare Analytics

Access via https://dash.cloudflare.com:
- Requests per second
- Bandwidth usage (includes S3 media CDN)
- Cache hit ratio
- Security threats blocked
- Response time (p50, p95, p99)

## Cost Estimate

Monthly production costs:
- **PostgreSQL RDS** (db.t4g.small, Multi-AZ, 50 GB): ~$28
- **Redis ElastiCache** (cache.t4g.small, 2 nodes): ~$24
- **ECS Fargate** (2 x 0.5 vCPU, 1 GB): ~$30
- **Application Load Balancer**: ~$18
- **S3 Storage** (100 GB, Standard): ~$2.30
- **S3 Requests** (1M GET, 100K PUT): ~$0.50
- **Data Transfer** (100 GB out): ~$9
- **Cloudflare**: Free
- **Total**: **~$112/month**

## Troubleshooting

### S3 Media Upload Failing

**Symptom**: 403 Forbidden when uploading media in Wagtail admin

**Solutions**:
1. Verify ECS task IAM role has S3 permissions
2. Check S3 bucket policy allows ECS task role
3. Verify `USE_S3_MEDIA=True` in environment
4. Check `AWS_STORAGE_BUCKET_NAME` matches actual bucket

### Media Not Serving

**Symptom**: 404 errors on media URLs

**Solutions**:
1. Verify media was uploaded to S3: `aws s3 ls s3://cineos-media-prod/`
2. Check S3 bucket is publicly readable (or signed URLs configured)
3. Verify `AWS_S3_CUSTOM_DOMAIN` is set correctly
4. Check Cloudflare is proxying correctly

### Database Connection Errors

**Symptom**: "could not connect to server" in logs

**Solutions**:
1. Verify RDS endpoint matches DATABASE_URL
2. Check RDS security group allows ECS security group (port 5432)
3. Verify VPC subnets are correct
4. Test connection from ECS task: `psql $DATABASE_URL`

## Security Best Practices

1. ✅ **S3 bucket versioning enabled** - Protect against accidental deletions
2. ✅ **S3 lifecycle policies** - Move old media to cheaper storage
3. ✅ **S3 CORS configured** - Only allow uploads from cineos.io
4. ✅ **ECS task IAM role** - Least-privilege S3 access
5. ✅ **Secrets Manager** - All credentials stored securely
6. ✅ **Multi-AZ deployment** - High availability
7. ✅ **Cloudflare WAF** - DDoS and bot protection
8. ✅ **Encryption at rest** - RDS, Redis, S3
9. ✅ **Encryption in transit** - TLS 1.2+ everywhere

## Next Steps

1. Build production Docker image with WhiteNoise for static files
2. Configure GitHub Actions deployment workflow
3. Set up CloudWatch alarms for critical metrics
4. Configure S3 lifecycle policies for cost optimization
5. Enable Cloudflare WAF rules
6. Set up backup verification process
7. Create disaster recovery runbook

## References

- [Wagtail S3 Documentation](https://docs.wagtail.org/en/stable/advanced_topics/deploying.html#cloud-storage)
- [django-storages S3 Backend](https://django-storages.readthedocs.io/en/latest/backends/amazon-S3.html)
- [Pegasus Production Deployment](https://docs.saaspegasus.com/deployment/)
- [AWS ECS Best Practices](https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/)
